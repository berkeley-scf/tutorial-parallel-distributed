<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Parallel Processing for Distributed Computing  in R, Python, Matlab, and C</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Parallel Processing for Distributed Computing  in R, Python, Matlab, and C</h1>

<h2>Parallelization tools in a distributed memory (multiple machine) context</h2>

<p>Chris Paciorek, Department of Statistics, UC Berkeley</p>

<h1>0) This Tutorial</h1>

<p>This tutorial covers strategies for using parallel processing in R, Python, Matlab (briefly), and C on multiple machines, in which the various processes must interact across a network linking the machines. </p>

<p>In the future, a screencast of the material may accompany this document.</p>

<p>We&#39;ll use a virtual machine developed here at Berkeley, <a href="http://bce.berkeley.edu">the Berkeley Common Environment (BCE)</a>. BCE is a virtual Linux machine - basically it is a Linux computer that you can run within your own computer, regardless of whether you are using Windows, Mac, or Linux. This provides a common environment so that things behave the same for all of us. At the moment, I advise using the BCE-2015-spring VM and starting up a virtual cluster using <em>StarCluster</em>, following the instructions at the <a href="http://bce.berkeley.edu/install.html">BCE install page</a>. Starting up BCE-based clusters using BCE-2015-fall is in flux at the moment; we hope to firm this up during fall semester, 2015.</p>

<p>This tutorial assumes you have a working knowledge of either R, Python, or C. </p>

<p>Materials for this tutorial, including the R markdown file and associated code files that were used to create this document are available on Github at (<a href="https://github.com/berkeley-scf/tutorial-parallel-distributed">https://github.com/berkeley-scf/tutorial-parallel-distributed</a>).  You can download the files by doing a git clone from a terminal window on a UNIX-like machine, as follows:</p>

<pre><code class="r">git clone https://github.com/berkeley-scf/tutorial-parallel-distributed
</code></pre>

<p>To create this HTML document, simply compile the corresponding R Markdown file in R as follows (the following will work from within BCE after cloning the repository as above).</p>

<pre><code class="r">Rscript -e &quot;library(knitr); knit2html(&#39;parallel-dist.Rmd&#39;)&quot;
</code></pre>

<p>This tutorial by Christopher Paciorek is licensed under a Creative Commons Attribution 3.0 Unported License.</p>

<h1>1) Types of parallel processing</h1>

<p>There are two basic flavors of parallel processing (leaving aside
GPUs): distributed memory and shared memory. With shared memory, multiple
processors (which I&#39;ll call cores) share the same memory. With distributed
memory, you have multiple nodes, each with their own memory. You can
think of each node as a separate computer connected by a fast network. </p>

<h2>1.1) Some useful terminology:</h2>

<ul>
<li><em>cores</em>: We&#39;ll use this term to mean the different processing
units available on a single node.</li>
<li><em>nodes</em>: We&#39;ll use this term to mean the different computers,
each with their own distinct memory, that make up a cluster or supercomputer.</li>
<li><em>processes</em>: computational tasks executing on a machine; multiple
processes may be executing at once. A given program may start up multiple
processes at once. Ideally we have no more processes than cores on
a node.</li>
<li><em>threads</em>: multiple paths of execution within a single process;
the OS sees the threads as a single process, but one can think of
them as &#39;lightweight&#39; processes. Ideally when considering the processes
and their threads, we would have no more processes and threads combined
than cores on a node.</li>
<li><em>forking</em>: child processes are spawned that are identical to
the parent, but with different process IDs and their own memory.</li>
<li><em>sockets</em>: some of R&#39;s parallel functionality involves creating
new R processes (e.g., starting processes via <em>Rscript</em>) and
communicating with them via a communication technology called sockets.</li>
</ul>

<h2>1.2) Distributed memory and an overview of the topics in this tutorial</h2>

<p>Parallel programming for distributed memory parallelism requires passing
messages between the different nodes. The standard protocol for doing
this is MPI, of which there are various versions, including <em>openMPI</em>, which we&#39;ll use here.</p>

<p>The R package <em>Rmpi</em> implements MPI in R. The <em>pbdR</em> packages for R also implement MPI as well as distributed linear algebra.</p>

<p>Python has a package <em>mpi4py</em> that allows use of MPI within Python.</p>

<p>In both R and Python, there are also easy ways to do embarrassingly parallel calculations (such as simple parallel for loops) across multiple machines, with MPI and similar tools used behind the scenes to manage the worker processes.</p>

<p>Matlab has its own system for distributed computation, called the Distributed Computing Server (DCS), requiring additional licensing above the standard Matlab installation. </p>

<p>This tutorial will cover:</p>

<ul>
<li>simple parallelization of embarrassingly parallel computations (in R, Python, and Matlab) without writing code that explicitly uses MPI;</li>
<li>distributed linear algebra using the pbdR front-end to the <em>ScaLapack</em> package; and</li>
<li>using MPI explicitly (in R, Python and C).</li>
</ul>

<h2>1.3) Other type of parallel processing</h2>

<p>We won&#39;t cover any of these in this material.</p>

<h3>Shared memory parallelization</h3>

<p>For shared memory parallelism, each core is accessing the same memory
so there is no need to pass information (in the form of messages)
between different machines. But in some programming contexts one needs
to be careful that activity on different cores doesn&#39;t mistakenly
overwrite places in memory that are used by other cores. Threading is a form of shared memory parallelism.</p>

<p>This tutorial will not cover shared memory parallelization, as it is covered in <a href="https://github.com/berkeley-scf/tutorial-parallel-basics">a separate tutorial</a>.</p>

<p>For information about working with random numbers in a parallel computation, please see that same tutorial, as the discussion applies to both shared and distributed memory. </p>

<h3>GPUs</h3>

<p>GPUs (Graphics Processing Units) are processing units originally designed
for rendering graphics on a computer quickly. This is done by having
a large number of simple processing units for massively parallel calculation.
The idea of general purpose GPU (GPGPU) computing is to exploit this
capability for general computation. </p>

<p>In spring 2014, I gave a <a href="http://statistics.berkeley.edu/computing/gpu">workshop on using GPUs</a>.</p>

<h3>Spark and Hadoop</h3>

<p>Spark and Hadoop are systems for implementing computations in a distributed
memory environment, using the MapReduce approach. In fall 2014, I gave a <a href="http://statistics.berkeley.edu/computing/gpu">workshop on using Spark</a>.</p>

<h1>2) Starting MPI-based jobs</h1>

<p>Code that explicitly uses MPI, as well as code using MPI under the hood, such as <em>foreach</em> with <em>doMPI</em> in R and pbdR, requires that you start your process(es) in a special way via the <em>mpirun</em> command. Note that <em>mpirun</em>, <em>mpiexec</em> and <em>orterun</em> are synonyms under <em>openMPI</em>. </p>

<p>The basic requirements for starting such a job are that you specify the number of processes you want to run and that you indicate what machines those processes should run on. Those machines should be networked together such that MPI can ssh to the various machines without any password required.</p>

<p>There are two ways to tell <em>mpirun</em> the machines on which to run the worker processes.</p>

<p>First, we can pass the machine names directly, replicating the name
if we want multiple processes on a single machine. Note that on a Starcluster-based EC2 VM, the nodes are named master, node001, node002, etc., and the .hosts file will already exist if you follow the BCE/Starcluster instructions given on the BCE install page. </p>

<pre><code class="bash">mpirun --host smeagol,radagast,arwen,arwen -np 4 hostname
</code></pre>

<pre><code>## smeagol
## radagast
## arwen
## arwen
</code></pre>

<p>Alternatively, we can create a file with the relevant information.</p>

<pre><code class="bash">echo &#39;smeagol slots=1&#39; &gt; .hosts
echo &#39;radagast slots=1&#39; &gt;&gt; .hosts
echo &#39;arwen slots=2&#39; &gt;&gt; .hosts
mpirun -machinefile .hosts -np 4 hostname
</code></pre>

<pre><code>## smeagol
## radagast
## arwen
## arwen
</code></pre>

<p>(I&#39;ve been having some MPI difficulties on a few of our servers, so in some of the results later in this tutorial, you&#39;ll see that the examples may use different machines than those indicated here.)</p>

<p>An alternative is just to manually duplicate host names to indicate the number of slots (though this may not work exactly with all versions of openMPI):</p>

<pre><code>echo -e &#39;smeagol\nradagast\narwen\narwen&#39; &gt; .hosts
cat .hosts
</code></pre>

<p>To limit the number of threads for each process, we can tell <em>mpirun</em>
to export the value of <em>OMP_NUM_THREADS</em> to the processes. E.g., calling a C program, <em>quad_mpi</em>:</p>

<pre><code>export OMP_NUM_THREADS=2
mpirun -machinefile .hosts -np 4 -x OMP_NUM_THREADS quad_mpi
</code></pre>

<p>In the examples above, I illustrated with a bash command and with a compiled C program, but one would similarly
use the -machinefile flag when starting R or Python or a C program via mpirun.</p>

<p>There are additional details involved in carefully controlling how processes are allocated to nodes, but the default arguments for mpirun should do a reasonable job in many situations. </p>

<p>Also, I&#39;ve had inconsistent results in terms of having the correct number of workers start up on each of the machines specified, depending on whether I specify the number of workers implicitly via the hosts information, explicitly via -np or both. You may want to check that the right number of workers is running on each host. </p>

<h1>3) Basic parallelization across nodes</h1>

<p>Here we&#39;ll see the use of high-level packages in R, Python, and Matlab that hide the details of communication between nodes. </p>

<h2>3.1) R</h2>

<h3>3.1.1) <em>foreach</em> with the <em>doMPI</em> backend</h3>

<p>Just as we used <em>foreach</em> in a shared memory context, we can
use it in a distributed memory context as well, and R will handle
everything behind the scenes for you. </p>

<p>Start R through the <em>mpirun</em> command as discussed above, either
as a batch job or for interactive use. We&#39;ll only ask for 1 process
because the worker processes will be started automatically from within R (but using the machine names information passed to mpirun).</p>

<pre><code>mpirun -machinefile .hosts -np 1 R CMD BATCH -q --no-save doMPI.R doMPI.out
mpirun -machinefile .hosts -np 1 R --no-save
</code></pre>

<p>Here&#39;s R code for using <em>Rmpi</em> as the back-end to <em>foreach</em>.
If you call <em>startMPIcluster</em> with no arguments, it will start
up one fewer worker processes than the number of hosts times slots given to mpirun
so your R code will be more portable. </p>

<pre><code class="r">## you should have invoked R as:
## mpirun -machinefile .hosts -np 1 R CMD BATCH --no-save file.R file.out

library(Rmpi)
library(doMPI)

cl = startMPIcluster()  # by default will start one fewer slave
# than elements in .hosts

registerDoMPI(cl)
clusterSize(cl) # just to check

results &lt;- foreach(i = 1:200) %dopar% {
  out = mean(rnorm(1e6))
}

closeCluster(cl)

mpi.quit()
</code></pre>

<pre><code class="bash">mpirun -machinefile .hosts -np 1 R CMD BATCH -q --no-save doMPI.R doMPI.out
cat doMPI.out
</code></pre>

<pre><code>## &gt; ## @knitr doMPI
## &gt; 
## &gt; ## you should have invoked R as:
## &gt; ## mpirun -machinefile .hosts -np 1 R CMD BATCH --no-save file.R file.out
## &gt; 
## &gt; library(Rmpi)
## &gt; library(doMPI)
## Loading required package: foreach
## Loading required package: iterators
## &gt; 
## &gt; cl = startMPIcluster()  # by default will start one fewer slave
##  3 slaves are spawned successfully. 0 failed.
## &gt; # than elements in .hosts
## &gt;                                         
## &gt; registerDoMPI(cl)
## &gt; clusterSize(cl) # just to check
## [1] 3
## &gt; 
## &gt; results &lt;- foreach(i = 1:200) %dopar% {
## +   out = mean(rnorm(1e6))
## + }
## &gt; 
## &gt; closeCluster(cl)
## &gt; 
## &gt; mpi.quit()
</code></pre>

<p>A caution concerning Rmpi/doMPI: when you invoke <code>startMPIcluster()</code>,
all the slave R processes become 100% active and stay active until
the cluster is closed. In addition, when <em>foreach</em> is actually
running, the master process also becomes 100% active. So using this
functionality involves some inefficiency in CPU usage. This inefficiency
is not seen with a sockets cluster (Section 3.1.4) nor when using other
Rmpi functionality - i.e., starting slaves with <em>mpi.spawn.Rslaves</em>
and then issuing commands to the slaves.</p>

<p>If you specified <code>-np</code> with more than one process then as with the C-based
MPI job above, you can control the threading via OMP_NUM_THREADS
and the -x flag to <em>mpirun</em>. Note that this only works when the
R processes are directly started by <em>mpirun</em>, which they are
not if you set -np 1. The <em>maxcores</em> argument to <em>startMPIcluster()</em>
does not seem to function (perhaps it does on other systems).</p>

<p>Sidenote: You can use <em>doMPI</em> on a single node, which might be useful for avoiding
some of the conflicts between R&#39;s forking functionality and openBLAS that
can cause R to hang when using <em>foreach</em> with <em>doParallel</em>.</p>

<h3>3.1.2) Using pbdR</h3>

<p>There is a relatively new effort to enhance R&#39;s capability for distributed
memory processing called <a href="http://r-pbd.org">pbdR</a>. For an extensive tutorial, see the
<a href="https://github.com/wrathematics/pbdDEMO/blob/master/inst/doc/pbdDEMO-guide.pdf?raw=true">pbdDEMO vignette</a>.
 <em>pbdR</em> is designed for
SPMD processing in batch mode, which means that you start up multiple
processes in a non-interactive fashion using mpirun. The same code
runs in each R process so you need to have the code behavior depend
on the process ID.</p>

<p><em>pbdR</em> provides the following capabilities:</p>

<ul>
<li>the ability to do some parallel apply-style computations (this section),</li>
<li>the ability to do distributed linear algebra by interfacing to <em>ScaLapack</em> (see Section 4), and</li>
<li>an alternative to <em>Rmpi</em> for interfacing with MPI (see Section 5).</li>
</ul>

<p>Personally, I think the second of the three is the most exciting as
it&#39;s a functionality not readily available in R or even more generally
in other readily-accessible software.</p>

<p>Let&#39;s see parallel-apply style computations in pbdR.</p>

<p>Here&#39;s some basic syntax for doing a distributed <em>apply()</em> on
a matrix that is on one of the workers. So in this case, the matrix is not initially distributed to the workers &ndash; that is done as part of the <em>pbdApply</em> computation. (One can also use <em>pbdApply</em> on matrices that are already distributed, and this is of course recommended for large matrices &ndash; see Section 4.) </p>

<p>As mentinoed above, pbdR code is always run in batch mode, with the same code running on all of the processes. This means that you often need to explicitly build in logic about which process should execute a given piece of code, including print statements. Here the check for <code>comm.rank() == 0</code> allows us to only create the matrix and call some print statements on the master node (rank 0).</p>

<pre><code class="r"># invoke as mpirun -machinefile .hosts -np 4 Rscript pbd-apply.R &gt; pbd-apply.out

library(pbdMPI, quiet = TRUE )
init()

nrows &lt;- 1e6

if(comm.rank()==0) {
    x &lt;- matrix(rnorm(nrows*50), nrow = nrows)
}

sm &lt;- comm.timer(out &lt;- pbdApply(x, 1, mean, pbd.mode = &#39;mw&#39;, rank.source = 0))
if(comm.rank()==0) {
    print(out[1:5])
    print(sm)
}

finalize()
</code></pre>

<pre><code class="bash">mpirun -machinefile .hosts -np 4 Rscript pbd-apply.R &gt; pbd-apply.out
cat pbd-apply.out
</code></pre>

<pre><code>## [1] -0.05169168 -0.05552842 -0.22260412 -0.21349725 -0.11454628
##      min     mean      max 
##  7.75200 17.31525 20.52700
</code></pre>

<h3>3.1.3) Using parallel apply functionality in Rmpi</h3>

<p><em>Rmpi</em> is a package that provides MPI capabilities from R, including low-level MPI type calls (see Section 5). It also provides high-level wrapper functions that use MPI behind the scenes, including parallel apply functionality for operating on lists (and vectors) with functions such as <em>mpi.parSapply</em>. </p>

<p>The documentation (see <code>help(mpi.parSapply)</code>) documents a number of confusingly-named functions. It appears that they are basically multi-node versions of the analogous <em>parSapply</em> and related functions. </p>

<pre><code class="r">library(Rmpi)
mpi.spawn.Rslaves()

myfun &lt;- function(i) {
      set.seed(i)
      mean(rnorm(1e7))
}

x &lt;- seq_len(25)
# parallel sapply-type calculations on a vector 
system.time(out &lt;- mpi.parSapply(x, myfun))
system.time(out &lt;- mpi.applyLB(x, myfun))

nrows &lt;- 10000
x &lt;- matrix(rnorm(nrows*50), nrow = nrows)
# parallel apply on a matrix
out &lt;- mpi.parApply(x, 1, mean)

mpi.close.Rslaves()
mpi.quit()
</code></pre>

<pre><code class="bash">mpirun -machinefile .hosts -np 1 R CMD BATCH -q --no-save mpi.parSapply.R mpi.parSapply.out
cat mpi.parSapply.out
</code></pre>

<pre><code>## &gt; ## @knitr mpi.parSapply
## &gt; 
## &gt; library(Rmpi)
## &gt; mpi.spawn.Rslaves()
##  4 slaves are spawned successfully. 0 failed.
## master (rank 0, comm 1) of size 5 is running on: scf-sm10 
## slave1 (rank 1, comm 1) of size 5 is running on: scf-sm10 
## slave2 (rank 2, comm 1) of size 5 is running on: scf-sm10 
## slave3 (rank 3, comm 1) of size 5 is running on: scf-sm11 
## slave4 (rank 4, comm 1) of size 5 is running on: scf-sm11 
## &gt; 
## &gt; myfun &lt;- function(i) {
## +       set.seed(i)
## +       mean(rnorm(1e7))
## + }
## &gt; 
## &gt; x &lt;- seq_len(25)
## &gt; # parallel sapply-type calculations on a vector 
## &gt; system.time(out &lt;- mpi.parSapply(x, myfun))
##    user  system elapsed 
##   5.997   5.225  11.463 
## &gt; system.time(out &lt;- mpi.applyLB(x, myfun))
##    user  system elapsed 
##   6.548   4.981  11.638 
## &gt; 
## &gt; nrows &lt;- 10000
## &gt; x &lt;- matrix(rnorm(nrows*50), nrow = nrows)
## &gt; # parallel apply on a matrix
## &gt; out &lt;- mpi.parApply(x, 1, mean)
## &gt; 
## &gt; mpi.close.Rslaves()
## [1] 1
## &gt; mpi.quit()
</code></pre>

<p>In some cases, it may be useful <em>job.num</em> when the number of tasks is bigger than the number of worker processes to ensure load-balancing.</p>

<h3>3.1.4) Using sockets</h3>

<p>One can also set up a cluster with the worker processes communicating via sockets. You just need to specify
a character vector with the machine names as the input to <em>makeCluster()</em>. A nice thing about this is that it doesn&#39;t involve any of the complications of working with needing MPI installed.</p>

<pre><code class="r">library(parallel)

machines = c(rep(&quot;smeagol.berkeley.edu&quot;, 1),
    rep(&quot;gandalf.berkeley.edu&quot;, 1),
    rep(&quot;arwen.berkeley.edu&quot;, 2))
cl = makeCluster(machines)
cl
</code></pre>

<pre><code>## socket cluster with 4 nodes on hosts &#39;smeagol.berkeley.edu&#39;, &#39;gandalf.berkeley.edu&#39;, &#39;arwen.berkeley.edu&#39;
</code></pre>

<pre><code class="r">n = 1e7
clusterExport(cl, c(&#39;n&#39;))
fun = function(i)
  out = mean(rnorm(n))

result &lt;- parSapply(cl, 1:20, fun)

result[1:5]
</code></pre>

<pre><code>## [1]  4.172127e-05  1.300298e-04 -7.128446e-04 -4.530147e-04 -3.676025e-04
</code></pre>

<pre><code class="r">stopCluster(cl) # not strictly necessary
</code></pre>

<h3>3.1.5) The <em>partools</em> package</h3>

<p><em>partools</em> is a new package developed by Norm Matloff at UC-Davis. He has the perspective that Spark/Hadoop are not the right tools in many cases when doing statistics-related work and has developed some simple tools for parallelizing computation across multiple nodes, also referred to as <em>Snowdoop</em>. The tools make use of the key idea in Hadoop of a distributed file system and distributed data objects but avoid the complications of trying to ensure fault tolerance, which is critical only on very large clusters of machines.</p>

<p>I haven&#39;t yet had time to develop any material based on <em>partools</em> but hope to in the future. </p>

<h2>3.2) Python</h2>

<p>One way to parallelize tasks across nodes in Python is using the pp package (also useful for parallelizing on a single machine as discussed in the <a href="https://github.com/berkeley-scf/tutorial-parallel-basics">parallel basics tutorial</a>. </p>

<p>Assuming that the pp package is installed on each node (e.g., <code>sudo apt-get install install python-pp</code> on an Ubuntu machine), you need to start up a ppserver process on each node. E.g., if <code>$nodes</code> is a UNIX environment variable containing the names of the worker nodes and you want to start 2 workers per node:</p>

<pre><code class="bash">nodes=&#39;smeagol radagast beren arwen&#39;
for node in $nodes; do
# cd /tmp is because of issue with starting ppserver in home directory
# -w says how many workers to start on the node
    ssh $node &quot;cd /tmp &amp;&amp; ppserver -s mysecretphrase -t 120 -w 2 &amp;&quot; &amp; 
done
</code></pre>

<p>Now in our Python code we create a server object and submit jobs to the server object, which manages the farming out of the tasks. Note that this will run interactively in iPython or as a script from UNIX, but there have been times where I was not able to run it interactively in the base Python interpreter. Also note that while we are illustrating this as basically another parallelized for loop, the individual jobs can be whatever calculations you want, so the  function (in this case it&#39;s always <em>pi.sample</em>) could change from job to job.</p>

<pre><code class="python">import numpy.random
import pp
import time
import pi_code # provided in pi_code.py

samples_per_slice = 10000000
num_slices = 24*20

# remember to start ppserver on worker nodes

# assume &#39;hosts&#39; contains the names of the nodes on which you 
# started ppserver
nprocsPerNode = 2
hosts = [&#39;smeagol&#39;, &#39;radagast&#39;, &#39;beren&#39;, &#39;arwen&#39;]
ppservers = hosts * nprocsPerNode

print ppservers
# put ncpus=0 here or it will start workers locally too
job_server = pp.Server(ncpus = 0, ppservers = tuple(ppservers), secret = &#39;mysecretphrase&#39;)

inputs = [(i, samples_per_slice) for i in xrange(num_slices)]

t0 = time.time()
jobs = [job_server.submit(pi_code.sample, invalue, modules = (&#39;numpy.random&#39;,)) for invalue in inputs]
results = [job() for job in jobs]
t1 = time.time()

print &quot;Pi is roughly %f&quot; % (4.0 * sum(results) / (num_slices*samples_per_slice))
print &quot;Time elapsed: &quot;, t1 - t0
</code></pre>

<pre><code class="bash">python python-pp.py &gt; python-pp.out
cat python-pp.out
</code></pre>

<pre><code>[&#39;smeagol&#39;, &#39;radagast&#39;, &#39;beren&#39;, &#39;arwen&#39;, &#39;smeagol&#39;, &#39;radagast&#39;, &#39;beren&#39;, &#39;arwen&#39;]
Pi is roughly 3.141567
Time elapsed:  32.0389587879
</code></pre>

<p>The -t flag used when starting ppserver should ensure that the server processes are removed, but if you need to do it manually, this should work:</p>

<pre><code class="bash">for node in $nodes; do
    killall ppserver
done
</code></pre>

<p><em>ipython</em> also provides parallelization capabilities. <a href="https://ipython.org/ipython-doc/3/parallel/index.html">Here is some info from the IPython documentation</a>, but I haven&#39;t explored this yet. </p>

<h2>3.3) Matlab</h2>

<p>To use Matlab across multiple nodes, you need to have the Matlab Distributed Computing Server (DCS). If it is installed, one can set up Matlab so that <em>parfor</em> will distribute its work across multiple nodes. Details may vary depending on how DCS is installed on your system. </p>

<h1>4) Distributed linear algebra in R using pbdR</h1>

<h2>4.1) Distributed linear algebra example</h2>

<p>And here&#39;s how you would set up a distributed matrix and do linear
algebra on it. Note that when working with large matrices, you would
generally want to construct the matrices (or read from disk) in a
parallel fashion rather than creating the full matrix on one worker.
For simplicity in the example, I construct the matrix, <em>x</em>,  on the master
and then create the distributed version of the matrix, <em>dx</em>, with <em>as.ddmatrix</em>.</p>

<p>Here&#39;s the code in <em>pbd-linalg.R</em>.</p>

<pre><code class="r">library(pbdDMAT, quiet = TRUE )

n &lt;- 4096*2

# if you are putting multiple processes on node
# you may want to prevent threading of the linear algebra:
# library(RhpcBLASctl)
# blas_set_num_threads(1)
# (or do by passing OMP_NUM_THREADS to mpirun

init.grid()

if(comm.rank()==0) print(date())

# pbd allows for parallel I/O, but here
# we keep things simple and distribute
# an object from one process
if(comm.rank() == 0) {
    x &lt;- rnorm(n^2)
    dim(x) &lt;- c(n, n)
} else x &lt;- NULL
dx &lt;- as.ddmatrix(x)

timing &lt;- comm.timer(sigma &lt;- crossprod(dx))

if(comm.rank()==0) {
    print(date())
    print(timing)
}

timing &lt;- comm.timer(out &lt;- chol(sigma))

if(comm.rank()==0) {
    print(date())
    print(timing)
}

finalize()
</code></pre>

<p>As before we run the job in batch mode via mpirun:</p>

<pre><code class="bash">export OMP_NUM_THREADS=1
mpirun -machinefile .hosts -np 4 -x OMP_NUM_THREADS Rscript pbd-linalg.R &gt; pbd-linalg.out
cat pbd-linalg.out
</code></pre>

<pre><code>## Using 2x2 for the default grid size
## 
## [1] &quot;Sat Oct 17 12:05:38 2015&quot;
## [1] &quot;Sat Oct 17 12:06:54 2015&quot;
##    min   mean    max 
## 48.086 50.806 52.585 
## [1] &quot;Sat Oct 17 12:08:10 2015&quot;
##      min     mean      max 
## 76.47000 76.51125 76.53300
</code></pre>

<p>You may want to set the <em>bldim</em> argument to <em>as.ddmatrix</em>. That determines
the size of the submatrices (aka &#39;blocks&#39;) into which the overall matrix is split. Generally, multiple
submatrices are owned by an individual worker process. For example, to use 100x100 
blocks, you&#39;d have</p>

<pre><code>dx &lt;- as.ddmatrix(x, bldim = c(100, 100))
</code></pre>

<p>In general, you don&#39;t
want the blocks too big as the work may not be well load-balanced, or too small as
that may have a higher computational cost in terms of latency and communication. 
My experiments suggest that it&#39;s worth exploring block sizes of 10x10 through 1000x1000 (if you have square matrices). </p>

<p>As a quick, completely non-definitive point of comparison, doing the
crossproduct and Cholesky for the 8192x8192 matrix on 3 EC2 nodes
(2 cores per node) with -np 6 took 39 seconds for each operation,
while doing with two threads on the master node took 64 seconds (crossproduct)
and 23 seconds (Cholesky). While that is a single test, some other experiments
I&#39;ve done also haven&#39;t show much speedup in using multiple nodes with pbdR compared
to simply using a threaded BLAS on one machine.  So you may need to get fairly big matrices
that won&#39;t fit in memory on a single machine before it&#39;s worthwhile 
to do the computation in distributed fashion using pbdR.</p>

<h2>4.2) Constructing a distributed matrix on parallel</h2>

<p>pbdR has functionality for reading in parallel from a parallel file
system such as Lustre (available on Berkeley&#39;s Savio cluster).  Things
are bit more complicated if that&#39;s not the case. Here&#39;s some code that
illustrates how to construct a distributed matrix from constituent column blocks.
 First create a distributed version of the
matrix using a standard R matrix with each process owning a block of
columns (I haven&#39;t yet gotten the syntax to work for blocks of rows). Then create a
pbd version of that distributed matrix and finally convert the
distributed matrix to a standard pbd block structure on which the
linear algebra can be done efficiently. </p>

<pre><code class="r">library(pbdDMAT, quiet = TRUE)
init.grid()

nprocs &lt;- comm.size()

nrows &lt;- 10000
ncolsPerBlock &lt;- nrows/nprocs

# each process has a block of columns as an R matrix
subdata &lt;- matrix(rnorm(nrows * ncolsPerBlock), ncol = ncols)

# now construct the distributed matrix object
tmp &lt;- ddmatrix(data = subdata, nrow = nrows, ncol = nrows,
               bldim = c(nrows, ncolsPerBlock), ICTXT = 1)
# now rearrange the blocks for better linear algebra performance
dx &lt;- redistribute(tmp, bldim = c(100, 100), ICTXT = 0)

finalize ()
</code></pre>

<p>The code above creates the submatrices within the R sessions, but one could also read in from separate files, one per process.</p>

<p>The code in <em>redistribute-test.R</em> demonstrates that constructing the full matrix 
from column-wise blocks with this syntax works correctly. </p>

<h1>5) MPI</h1>

<h2>5.1) MPI Overview</h2>

<p>There are multiple MPI implementations, of which <em>openMPI</em> and
<em>mpich</em> are very common. <em>openMPI</em> is on BCE, and we&#39;ll use that.</p>

<p>In MPI programming, the same code runs on all the machines. This is
called SPMD (single program, multiple data). As we saw a bit with the pbdR code, one
invokes the same code (same program) multiple times, but the behavior
of the code can be different based on querying the rank (ID) of the
process. Since MPI operates in a distributed fashion, any transfer
of information between processes must be done explicitly via send
and receive calls (e.g., <em>MPI_Send</em>, <em>MPI_Recv</em>, <em>MPI_Isend</em>,
and <em>MPI_Irecv</em>). (The &ldquo;MPI_&#39;&#39; is for C code; C++ just has
<em>Send</em>, <em>Recv</em>, etc.)</p>

<p>The latter two of these functions (<em>MPI_Isend</em> and <em>MPI_Irecv</em>)
are so-called non-blocking calls. One important concept to understand
is the difference between blocking and non-blocking calls. Blocking
calls wait until the call finishes, while non-blocking calls return
and allow the code to continue. Non-blocking calls can be more efficient,
but can lead to problems with synchronization between processes. </p>

<p>In addition to send and receive calls to transfer to and from specific
processes, there are calls that send out data to all processes (<em>MPI_Scatter</em>),
gather data back (<em>MPI_Gather</em>) and perform reduction operations
(<em>MPI_Reduce</em>).</p>

<p>Debugging MPI code can be tricky because communication
can hang, error messages from the workers may not be seen or readily
accessible, and it can be difficult to assess the state of the worker
processes. </p>

<h2>5.2) Basic syntax for MPI in C</h2>

<p>Here&#39;s a basic hello world example  The code is also in <em>mpiHello.c</em>.</p>

<pre><code>// see mpiHello.c
#include &lt;stdio.h&gt; 
#include &lt;math.h&gt; 
#include &lt;mpi.h&gt;

int main(int argc, char* argv) {     
    int myrank, nprocs, namelen;     
    char process_name[MPI_MAX_PROCESSOR_NAME];
    MPI_Init(&amp;argc, &amp;argv);     
    MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);   
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);          
    MPI_Get_processor_name(process_name, &amp;namelen);            
    printf(&quot;Hello from process %d of %d on %s\n&quot;, 
        myrank, nprocs, process_name);
    MPI_Finalize();     
    return 0; 
} 
</code></pre>

<p>There are C (<em>mpicc</em>) and C++ (<em>mpic++</em>) compilers for MPI programs (<em>mpicxx</em> and <em>mpiCC</em> are synonyms).
I&#39;ll use the MPI C++ compiler
even though the code is all plain C code.</p>

<pre><code class="bash">mpicxx mpiHello.c -o mpiHello
cat .hosts # what hosts do I expect it to run on?
mpirun -machinefile .hosts -np 4 mpiHello
</code></pre>

<pre><code>## smeagol slots=1
## radagast slots=1
## arwen slots=2
## Hello from processor 1 of 4 on radagast
## Hello from processor 2 of 4 on arwen
## Hello from processor 3 of 4 on arwen
## Hello from processor 0 of 4 on smeagol
</code></pre>

<p>To actually write real MPI code, you&#39;ll need to go learn some of the
MPI syntax. See <em>quad_mpi.c</em> and <em>quad_mpi.cpp</em>, which
are example C and C++ programs (for approximating an integral via
quadrature) that show some of the basic MPI functions. Compilation
and running are as above:</p>

<pre><code class="bash">mpicxx quad_mpi.cpp -o quad_mpi
mpirun -machinefile .hosts -np 4 quad_mpi
</code></pre>

<pre><code>## 17 October 2015 02:05:02 PM
## 
## QUAD_MPI
##   C++/MPI version
##   Estimate an integral of f(x) from A to B.
##   f(x) = 50 / (pi * ( 2500 * x * x + 1 ) )
## 
##   A = 0
##   B = 10
##   N = 999999999
##   EXACT =       0.4993633810764567
## 
##   Use MPI to divide the computation among 4 total processes,
##   of which one is the master and does not do core computations.
##   Process 3 contributed MY_TOTAL = 0.000318308
##   Process 2 contributed MY_TOTAL = 0.00095491
##   Process 1 contributed MY_TOTAL = 0.49809
## 
##   Estimate =       0.4993634591634721
##   Error = 7.808701535383378e-08
##   Time = 9.443312883377075
## 
## QUAD_MPI:
##   Normal end of execution.
## 
## 17 October 2015 02:05:11 PM
</code></pre>

<h2>5.3) Using MPI from R via Rmpi or pbdR</h2>

<h3>5.3.1) Rmpi</h3>

<p>R users can use Rmpi to interface with MPI. </p>

<p>Here&#39;s some example code that uses actual Rmpi syntax (as opposed
to <em>foreach</em> with Rmpi as the back-end, where the use of Rmpi was hidden from us).
The syntax is very similar to the MPI C syntax we&#39;ve already seen.
This code runs in a master-slave paradigm where the master starts
the slaves and invokes commands on them. It may be possible to run
Rmpi in a context where each process runs the same code based
on invoking with Rmpi, but I haven&#39;t investigated this further. </p>

<pre><code class="r"># example syntax of standard MPI functions

library(Rmpi)
# by default will start one fewer workers than processes
# saving one for the master
mpi.spawn.Rslaves()

n = 5
mpi.bcast.Robj2slave(n)
mpi.bcast.cmd(id &lt;- mpi.comm.rank())
mpi.bcast.cmd(x &lt;- rnorm(id))

mpi.remote.exec(ls(.GlobalEnv))

mpi.bcast.cmd(y &lt;- 2 * x)
mpi.remote.exec(print(y))

objs &lt;- as.list(c(&#39;x&#39;, &#39;n&#39;))
# next command sends value of objs on _master_ as argument to rm
mpi.remote.exec(do.call, rm, objs)

# verify that &#39;n&#39; is gone:
mpi.remote.exec(print(n))

# collect results back via send/recv
mpi.remote.exec(mpi.send.Robj(y, dest = 0, tag = 1))
results = list()
for(i in 1:(mpi.comm.size()-1)){
  results[[i]] = mpi.recv.Robj(source = i, tag = 1)
}

print(results)

mpi.close.Rslaves()
mpi.quit()
</code></pre>

<p><em>mpi.bcast.cmd</em> and  <em>mpi.remote.exec</em> are quite similar - they execute a function on the workers and can also use arguments on the master as inputs to the function evaluated on the workers (see the &hellip; argument). <em>mpi.remote.exec</em> can return the results of the execution to the master. </p>

<p>As before, we would start R via <em>mpirun</em>, requesting one process, since the workers are started within R via <em>mpi.spawn.Rslaves</em>.</p>

<pre><code class="bash">mpirun -machinefile .hosts -np 1 R CMD BATCH -q --no-save Rmpi.R Rmpi.out
cat Rmpi.out
</code></pre>

<pre><code>## &gt; ## @knitr Rmpi
## &gt; 
## &gt; # example syntax of standard MPI functions
## &gt; 
## &gt; library(Rmpi)
## &gt; # by default will start one fewer workers than processes
## &gt; # saving one for the master
## &gt; mpi.spawn.Rslaves()
##  4 slaves are spawned successfully. 0 failed.
## master (rank 0, comm 1) of size 5 is running on: scf-sm10 
## slave1 (rank 1, comm 1) of size 5 is running on: scf-sm10 
## slave2 (rank 2, comm 1) of size 5 is running on: scf-sm10 
## slave3 (rank 3, comm 1) of size 5 is running on: scf-sm11 
## slave4 (rank 4, comm 1) of size 5 is running on: scf-sm11 
## &gt; 
## &gt; n = 5
## &gt; mpi.bcast.Robj2slave(n)
## &gt; mpi.bcast.cmd(id &lt;- mpi.comm.rank())
## &gt; mpi.bcast.cmd(x &lt;- rnorm(id))
## &gt; 
## &gt; mpi.remote.exec(ls(.GlobalEnv))
## $slave1
## [1] &quot;id&quot; &quot;n&quot;  &quot;x&quot; 
## 
## $slave2
## [1] &quot;id&quot; &quot;n&quot;  &quot;x&quot; 
## 
## $slave3
## [1] &quot;id&quot; &quot;n&quot;  &quot;x&quot; 
## 
## $slave4
## [1] &quot;id&quot; &quot;n&quot;  &quot;x&quot; 
## 
## &gt; 
## &gt; mpi.bcast.cmd(y &lt;- 2 * x)
## &gt; mpi.remote.exec(print(y))
## $slave1
## [1] 2.52689
## 
## $slave2
## [1] -1.6820329  0.8934232
## 
## $slave3
## [1]  3.3031596 -2.3544619 -0.5147955
## 
## $slave4
## [1] -0.34978007 -0.03258057 -1.74705907 -2.20813628
## 
## &gt; 
## &gt; objs &lt;- as.list(c(&#39;x&#39;, &#39;n&#39;))
## &gt; # next command sends value of objs on _master_ as argument to rm
## &gt; mpi.remote.exec(do.call, rm, objs)
## $slave2
## [1] 0
## 
## &gt; 
## &gt; # verify that &#39;n&#39; is gone:
## &gt; mpi.remote.exec(print(n))
## $slave1
## [1] &quot;Error in print(n) : object &#39;n&#39; not found\n&quot;
## attr(,&quot;class&quot;)
## [1] &quot;try-error&quot;
## attr(,&quot;condition&quot;)
## &lt;simpleError in print(n): object &#39;n&#39; not found&gt;
## 
## $slave2
## [1] &quot;Error in print(n) : object &#39;n&#39; not found\n&quot;
## attr(,&quot;class&quot;)
## [1] &quot;try-error&quot;
## attr(,&quot;condition&quot;)
## &lt;simpleError in print(n): object &#39;n&#39; not found&gt;
## 
## $slave3
## [1] &quot;Error in print(n) : object &#39;n&#39; not found\n&quot;
## attr(,&quot;class&quot;)
## [1] &quot;try-error&quot;
## attr(,&quot;condition&quot;)
## &lt;simpleError in print(n): object &#39;n&#39; not found&gt;
## 
## $slave4
## [1] &quot;Error in print(n) : object &#39;n&#39; not found\n&quot;
## attr(,&quot;class&quot;)
## [1] &quot;try-error&quot;
## attr(,&quot;condition&quot;)
## &lt;simpleError in print(n): object &#39;n&#39; not found&gt;
## 
## &gt; 
## &gt; # collect results back via send/recv
## &gt; mpi.remote.exec(mpi.send.Robj(y, dest = 0, tag = 1))
## $slave1
##          used (Mb) gc trigger (Mb) max used (Mb)
## Ncells 223303 12.0     460000 24.6   350000 18.7
## Vcells 320694  2.5     786432  6.0   785990  6.0
## 
## $slave2
##          used (Mb) gc trigger (Mb) max used (Mb)
## Ncells 223303 12.0     460000 24.6   350000 18.7
## Vcells 320695  2.5     786432  6.0   785990  6.0
## 
## $slave3
##          used (Mb) gc trigger (Mb) max used (Mb)
## Ncells 223303 12.0     460000 24.6   350000 18.7
## Vcells 320697  2.5     786432  6.0   785990  6.0
## 
## $slave4
##          used (Mb) gc trigger (Mb) max used (Mb)
## Ncells 223303 12.0     460000 24.6   350000 18.7
## Vcells 320697  2.5     786432  6.0   785990  6.0
## 
## &gt; results = list()
## &gt; for(i in 1:(mpi.comm.size()-1)){
## +   results[[i]] = mpi.recv.Robj(source = i, tag = 1)
## + }
## &gt;   
## &gt; print(results)
## [[1]]
## [1] 2.52689
## 
## [[2]]
## [1] -1.6820329  0.8934232
## 
## [[3]]
## [1]  3.3031596 -2.3544619 -0.5147955
## 
## [[4]]
## [1] -0.34978007 -0.03258057 -1.74705907 -2.20813628
## 
## &gt; 
## &gt; mpi.close.Rslaves()
## [1] 1
## &gt; mpi.quit()
</code></pre>

<p>Note that if you do this in interactive mode, some of the usual functionality
of command line R (tab completion, scrolling for history) is not enabled
and errors will cause R to quit. This occurs because passing things
through <em>mpirun</em> causes R to think it is not running interactively.</p>

<p>Note: in some cases a cluster/supercomputer will be set up so that
<em>Rmpi</em> is loaded and the worker processes are already started
when you start R. In this case you wouldn&#39;t need to load <em>Rmpi</em>
or use <em>mpi.spawn.Rslaves</em>. You can always run <code>mpi.comm.size()</code> to see how
many workers are running.</p>

<h3>5.3.2) pbdMPI in pbdR</h3>

<p>Here&#39;s an example of distributing an embarrassingly parallel calculation
(estimating an integral via Monte Carlo - in this case estimating
the value of pi).</p>

<pre><code class="r">library(pbdMPI, quiet = TRUE )
init()

myRank &lt;- comm.rank() # comm index starts at 0 , not 1
comm.print(myRank , all.rank=TRUE)
node &lt;- system(&quot;cat /etc/hostname&quot;, intern = TRUE) # Sys.getenv(&quot;HOSTNAME&quot;)
if(myRank == 0) {
    comm.print(paste0(&quot;hello, world from &quot;, myRank, &quot; &quot;, node), all.rank=TRUE)
} else comm.print(paste0(&quot;goodbye from &quot;, myRank, &quot; &quot;, node), all.rank=TRUE)

if(comm.rank() == 0) print(date())
set.seed(myRank)  # see parallel basics tutorial for more on parallel random number generation
N.gbd &lt;- 1e7
X.gbd &lt;- matrix(runif(N.gbd * 2), ncol = 2)
r.gbd &lt;- sum(rowSums(X.gbd^2) &lt;= 1)
ret &lt;- allreduce(c(N.gbd,r.gbd), op = &quot;sum&quot;)
PI &lt;- 4 * ret [2] / ret [1]
comm.print(paste0(&quot;Pi is roughly: &quot;, PI))
if(comm.rank() == 0) print(date())

finalize()
</code></pre>

<pre><code class="bash">mpirun -machinefile .hosts -np 4 Rscript pbd-mpi.R &gt; pbd-mpi.out
cat pbd-mpi.out
</code></pre>

<pre><code>## COMM.RANK = 2
## [1] 2
## COMM.RANK = 3
## [1] 3
## COMM.RANK = 1
## [1] 1
## COMM.RANK = 0
## [1] 0
## COMM.RANK = 0
## [1] &quot;hello, world from 0 scf-sm10&quot;
## [1] &quot;Sat Oct 17 12:21:28 2015&quot;
## COMM.RANK = 3
## [1] &quot;goodbye from 3 scf-sm11&quot;
## COMM.RANK = 1
## [1] &quot;goodbye from 1 scf-sm10&quot;
## COMM.RANK = 2
## [1] &quot;goodbye from 2 scf-sm11&quot;
## COMM.RANK = 0
## [1] &quot;Pi is roughly: 3.1421032&quot;
## [1] &quot;Sat Oct 17 12:21:31 2015&quot;
</code></pre>

<h2>5.4) Using MPI from Python via mpi4py</h2>

<p>Here&#39;s some basic use of MPI within Python.</p>

<pre><code class="python">from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD

# simple print out Rank &amp; Size
id = comm.Get_rank()
print &quot;Of &quot;, comm.Get_size() , &quot; workers, I am number &quot; , id, &quot;.&quot;

def f(id, n):
    np.random.seed(id)
    return np.mean(np.random.normal(0, 1, n))

n = 1000000
result = f(id, n)


output = comm.gather(result, root = 0)

if id == 0:
    print output
</code></pre>

<p>To run the code, we start Python through the mpirun command as done previously.</p>

<pre><code class="bash">mpirun -machinefile .hosts -np 4 python example-mpi.py 
</code></pre>

<pre><code>## Of  4  workers, I am number  2 .
## Of  4  workers, I am number  3 .
## Of  4  workers, I am number  1 .
## Of  4  workers, I am number  0 .
## [0.0015121465155362318, 0.00065180430801923422, -0.000977212317921356, 0.001958404534987673]
</code></pre>

<p>More generally, you can send, receive, broadcast, gather, etc. as with MPI itself.</p>

<p><em>mpi4py</em> generally does not work interactively.</p>

<h1>6) Parallelization strategies</h1>

<p>The following are some basic principles/suggestions for how to parallelize
your computation.</p>

<p>Should I use one machine/node or many machines/nodes?</p>

<ul>
<li>If you can do your computation on the cores of a single node using
shared memory, that will be faster than using the same number of cores
(or even somewhat more cores) across multiple nodes. Similarly, jobs
with a lot of data/high memory requirements that one might think of
as requiring Spark or Hadoop may in some cases be much faster if you can find
a single machine with a lot of memory.</li>
<li>That said, if you would run out of memory on a single node, then you&#39;ll
need to use distributed memory.</li>
</ul>

<p>What level or dimension should I parallelize over?</p>

<ul>
<li>If you have nested loops, you generally only want to parallelize at
one level of the code. That said, there may be cases in which it is
helpful to do both. Keep in mind whether your linear algebra is being
threaded. Often you will want to parallelize over a loop and not use
threaded linear algebra.</li>
<li>Often it makes sense to parallelize the outer loop when you have nested
loops.</li>
<li>You generally want to parallelize in such a way that your code is
load-balanced and does not involve too much communication. </li>
</ul>

<p>How do I balance communication overhead with keeping my cores busy?</p>

<ul>
<li>If you have very few tasks, particularly if the tasks take different
amounts of time, often some of the processors will be idle and your code
poorly load-balanced.</li>
<li>If you have very many tasks and each one takes little time, the communication
overhead of starting and stopping the tasks will reduce efficiency.</li>
</ul>

<p>Should multiple tasks be pre-assigned to a process (i.e., a worker) (sometimes called <em>prescheduling</em>) or should tasks
be assigned dynamically as previous tasks finish? </p>

<ul>
<li>Basically if you have many tasks that each take similar time, you
want to preschedule the tasks to reduce communication. If you have few tasks
or tasks with highly variable completion times, you don&#39;t want to
preschedule, to improve load-balancing.</li>
<li>For R in particular, some of R&#39;s parallel functions allow you to say whether the 
tasks should be prescheduled. E.g., <code>library(Rmpi); help(mpi.parSapply)</code> gives some information.</li>
</ul>

</body>

</html>
